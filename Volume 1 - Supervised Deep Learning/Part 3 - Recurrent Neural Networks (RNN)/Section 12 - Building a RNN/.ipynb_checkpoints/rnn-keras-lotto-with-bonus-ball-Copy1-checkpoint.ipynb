{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we will try to find second ball given data from past 6 lotto and first ball\n",
    "\n",
    "\n",
    "# Import usual suspects\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5\n",
       "0   8  13  26  35  45  51\n",
       "1   1  15  24  31  34  44\n",
       "2   3   8  29  30  31  49\n",
       "3  21  25  39  50  54  59\n",
       "4  15  19  32  38  47  50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing  the training set\n",
    "pd_training_set = pd.read_csv('Lottery_NY_Lotto_Winning_Numbers__Beginning_2001_without_bonus.csv', header=None)\n",
    "pd_training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_train = []\n",
    "# take 300 rows less from total for testing and rest use for training\n",
    "# for col in pd_training_set.iloc[:-300,:].values:\n",
    "for col in pd_training_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_train.append([row])\n",
    "#         print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n",
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "# convert python list into numpy array\n",
    "training_set_val = np.array(two_dim_lotto_array_train, ndmin=2)\n",
    "\n",
    "print (type(training_set_val))\n",
    "print (training_set_val.ndim)\n",
    "print (training_set_val.shape)\n",
    "print (training_set_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Now, let's do normalization\n",
    "\n",
    "# # from sklearn.preprocessing import MinMaxScaler\n",
    "# # sc = MinMaxScaler()\n",
    "# # # Accuracy: 0.0317802805365\n",
    "\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# sc = MaxAbsScaler()\n",
    "# #  Accuracy: 0.0293846626395\n",
    "\n",
    "\n",
    "#  #Accuracy: 0.0697802805365\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# sc = RobustScaler()\n",
    "\n",
    "# training_set = sc.fit_transform(training_set_val)\n",
    "# print (training_set)\n",
    "# print (type(training_set))\n",
    "# print (training_set.ndim)\n",
    "# print (training_set.shape)\n",
    "\n",
    "# # ALSO TRY STANDARDIZATION INSTEAD OF NORMALIZATION\n",
    "\n",
    "# # from sklearn.preprocessing import StandardScaler\n",
    "# # sc = StandardScaler()\n",
    "\n",
    "# # training_set = sc.fit_transform(training_set_val)\n",
    "# # print (training_set)\n",
    "# # print (type(training_set))\n",
    "# # print (training_set.ndim)\n",
    "# # print (training_set.shape)\n",
    "\n",
    "# # NO NORMALIZATION\n",
    "training_set = training_set_val\n",
    "print (training_set)\n",
    "print (type(training_set))\n",
    "print (training_set.ndim)\n",
    "print (training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (9761, 7)\n",
      "X_train [[ 8 13 26 ..., 45 51  1]\n",
      " [13 26 35 ..., 51  1 15]\n",
      " [26 35 45 ...,  1 15 24]\n",
      " ..., \n",
      " [38 50 53 ...,  4  7 12]\n",
      " [50 53 57 ...,  7 12 17]\n",
      " [53 57  4 ..., 12 17 26]]\n",
      "y_train.shape (9761,)\n",
      "y_train [15 24 31 ..., 17 26 55]\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(7, 9768):\n",
    "    X_train.append(training_set[i-7:i, 0])\n",
    "    y_train.append(training_set[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "print (\"X_train.shape\", X_train.shape)\n",
    "print (\"X_train\", X_train)\n",
    "print (\"y_train.shape\", y_train.shape)\n",
    "print (\"y_train\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9762, 6, 1)\n",
      "[[[ 8]\n",
      "  [13]\n",
      "  [26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]]\n",
      "\n",
      " [[13]\n",
      "  [26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]\n",
      "  [ 1]]\n",
      "\n",
      " [[26]\n",
      "  [35]\n",
      "  [45]\n",
      "  [51]\n",
      "  [ 1]\n",
      "  [15]]\n",
      "\n",
      " ..., \n",
      " [[50]\n",
      "  [53]\n",
      "  [57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]]\n",
      "\n",
      " [[53]\n",
      "  [57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]\n",
      "  [17]]\n",
      "\n",
      " [[57]\n",
      "  [ 4]\n",
      "  [ 7]\n",
      "  [12]\n",
      "  [17]\n",
      "  [26]]]\n"
     ]
    }
   ],
   "source": [
    "# Now, Reshaping for keras before training, as it requires 3 dimenions\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Initializing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing the Keras liabraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", input_shape=(None, 1), return_sequences=True, units=6)`\n",
      "  \n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  import sys\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  \n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  if __name__ == '__main__':\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=6)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=6)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Create a regressor bassed upon sequential RNN\n",
    "# classifier = Sequential()\n",
    "# classifier.add(LSTM(units=10, activation='sigmoid', input_shape=(None, 1)))\n",
    "# classifier.add(Dense(units=1))\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', input_shape=(None, 1), return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid', return_sequences = True))\n",
    "regressor.add(LSTM(output_dim=6, activation='sigmoid'))\n",
    "regressor.add(Dense(output_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let' compile our RNN regressor\n",
    "# classifier.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "regressor.compile(optimizer='rmsprop', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda/envs/keras-playground/lib/python3.6/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9762/9762 [==============================] - 22s - loss: 1138.4247    \n",
      "Epoch 2/50\n",
      "9762/9762 [==============================] - 20s - loss: 1026.1702    \n",
      "Epoch 3/50\n",
      "9762/9762 [==============================] - 20s - loss: 952.5467    \n",
      "Epoch 4/50\n",
      "9762/9762 [==============================] - 20s - loss: 884.8630    \n",
      "Epoch 5/50\n",
      "9762/9762 [==============================] - 21s - loss: 821.1612    \n",
      "Epoch 6/50\n",
      "9762/9762 [==============================] - 20s - loss: 760.9052    \n",
      "Epoch 7/50\n",
      "9762/9762 [==============================] - 19s - loss: 704.3813    \n",
      "Epoch 8/50\n",
      "9762/9762 [==============================] - 18s - loss: 651.7109    \n",
      "Epoch 9/50\n",
      "9762/9762 [==============================] - 18s - loss: 602.4617    \n",
      "Epoch 10/50\n",
      "9762/9762 [==============================] - 18s - loss: 557.1613    \n",
      "Epoch 11/50\n",
      "9762/9762 [==============================] - 18s - loss: 515.5312    \n",
      "Epoch 12/50\n",
      "9762/9762 [==============================] - 19s - loss: 477.2605    \n",
      "Epoch 13/50\n",
      "9762/9762 [==============================] - 18s - loss: 442.7885    \n",
      "Epoch 14/50\n",
      "3600/9762 [==========>...................] - ETA: 12s - loss: 425.7577"
     ]
    }
   ],
   "source": [
    "# regressor.fit(X_train ,y_train, batch_size=32, epochs=50) with 4 LSTM and time 60 i achieved 0.0219 loss with t = 1\n",
    "\n",
    "regressor.fit(X_train ,y_train, batch_size=36, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor.save('lotto_regressor_5.1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "from keras.models import load_model\n",
    "model = load_model('lotto_regressor_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2   3   4   5\n",
       "0    8  13  26  35  45  51\n",
       "1    1  15  24  31  34  44\n",
       "2    3   8  29  30  31  49\n",
       "3   21  25  39  50  54  59\n",
       "4   15  19  32  38  47  50\n",
       "5   21  25  27  39  44  56\n",
       "6    3   8  13  21  33  41\n",
       "7    8  14  19  35  42  50\n",
       "8    1   6  23  30  40  51\n",
       "9    5  17  30  35  41  49\n",
       "10   5   8  21  24  50  56\n",
       "11   8  18  34  44  50  54\n",
       "12   3   8  25  31  38  44\n",
       "13   1  17  25  27  28  50\n",
       "14   1   2  32  33  36  39\n",
       "15   7  12  20  34  51  52\n",
       "16   8  18  21  22  26  49\n",
       "17   9  10  20  34  55  56\n",
       "18   6   8  11  17  22  41\n",
       "19  21  27  29  34  41  48"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Importing  the training set\n",
    "pd_testing_set = pd.read_csv('Lottery_NY_Lotto_Winning_Numbers__Beginning_2001_without_bonus.csv', header=None)\n",
    "pd_testing_set.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_test = []\n",
    "# take all rows less from total for testing and rest use for training\n",
    "for col in pd_testing_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_test.append([row])\n",
    "#         print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9768, 1)\n",
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [17]\n",
      " [26]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "# convert python list into numpy array\n",
    "testing_set = np.array(two_dim_lotto_array_test, ndmin=2)\n",
    "\n",
    "print (type(testing_set))\n",
    "print (testing_set.ndim)\n",
    "print (testing_set.shape)\n",
    "print (testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_actual.shape (9762,)\n",
      "y_actual [ 1 15 24 ..., 17 26 55]\n"
     ]
    }
   ],
   "source": [
    "y_actual = []\n",
    "for i in range(6, 9768):\n",
    "    y_actual.append(testing_set[i, 0])\n",
    "\n",
    "y_actual = np.array(y_actual)\n",
    "\n",
    "print (\"y_actual.shape\", y_actual.shape)\n",
    "print (\"y_actual\", y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9768, 1)\n",
      "inputs [[-0.73333333]\n",
      " [-0.56666667]\n",
      " [-0.13333333]\n",
      " ..., \n",
      " [-0.43333333]\n",
      " [-0.13333333]\n",
      " [ 0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "sc = RobustScaler()\n",
    "scaled_testing_set = sc.fit_transform(testing_set)\n",
    "\n",
    "# scaled_testing_set = testing_set\n",
    "\n",
    "print (\"inputs.shape\", scaled_testing_set.shape)\n",
    "print (\"inputs\", scaled_testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9762, 6)\n",
      "inputs [[-0.73333333 -0.56666667 -0.13333333  0.16666667  0.5         0.7       ]\n",
      " [-0.56666667 -0.13333333  0.16666667  0.5         0.7        -0.96666667]\n",
      " [-0.13333333  0.16666667  0.5         0.7        -0.96666667 -0.5       ]\n",
      " ..., \n",
      " [ 0.66666667  0.76666667  0.9        -0.86666667 -0.76666667 -0.6       ]\n",
      " [ 0.76666667  0.9        -0.86666667 -0.76666667 -0.6        -0.43333333]\n",
      " [ 0.9        -0.86666667 -0.76666667 -0.6        -0.43333333 -0.13333333]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(6, 9768):\n",
    "    inputs.append(scaled_testing_set[i-6:i, 0])\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "\n",
    "print (\"inputs.shape\", inputs.shape)\n",
    "print (\"inputs\", inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inputs.shape (9762, 6, 1)\n",
      "test_inputs [[[-0.73333333]\n",
      "  [-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]]\n",
      "\n",
      " [[-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]]\n",
      "\n",
      " [[-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]\n",
      "  [-0.5       ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.66666667]\n",
      "  [ 0.76666667]\n",
      "  [ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]]\n",
      "\n",
      " [[ 0.76666667]\n",
      "  [ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]]\n",
      "\n",
      " [[ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]\n",
      "  [-0.13333333]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape input to fit keras predicted model format of 3-d\n",
    "test_inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1],1))\n",
    "\n",
    "print (\"test_inputs.shape\", test_inputs.shape)\n",
    "print (\"test_inputs\", test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9762, 1)\n",
      "[[  6.91761398]\n",
      " [ 10.01141357]\n",
      " [ 20.58066559]\n",
      " ..., \n",
      " [ 20.81396103]\n",
      " [ 28.22393036]\n",
      " [ 39.52734375]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's predict\n",
    "predicted_lotto_numbers = model.predict(test_inputs)\n",
    "\n",
    "# convert prediction back to normal values \n",
    "predicted_lotto_numbers = sc.inverse_transform(predicted_lotto_numbers)\n",
    "\n",
    "print (type(predicted_lotto_numbers))\n",
    "print (predicted_lotto_numbers.ndim)\n",
    "print (predicted_lotto_numbers.shape)\n",
    "print (predicted_lotto_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "(9762,)\n",
      "[  7.  10.  21. ...,  21.  28.  40.]\n"
     ]
    }
   ],
   "source": [
    "# same for X_test\n",
    "y_predicted = np.around(predicted_lotto_numbers)\n",
    "y_predicted = np.reshape(y_predicted, (inputs.shape[0]))\n",
    "print (type(y_predicted))\n",
    "print (y_predicted.ndim)\n",
    "print (y_predicted.shape)\n",
    "print (y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "(9762,)\n",
      "[ 1 15 24 ..., 17 26 55]\n"
     ]
    }
   ],
   "source": [
    "print (type(y_actual))\n",
    "print (y_actual.ndim)\n",
    "print (y_actual.shape)\n",
    "print (y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ..., False False False]\n",
      "Accuracy: 0.0697602950215\n"
     ]
    }
   ],
   "source": [
    "true_predictions = np.equal(y_predicted, y_actual)\n",
    "print (true_predictions)\n",
    "print (\"Accuracy:\", np.sum(true_predictions)/np.size(true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part 4: Let's Visualize the results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxsHBzMxq/gYZKNnnPVZs1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d6cfc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_actual, color='red', label = 'Real Lotto Numbers')\n",
    "plt.plot(y_predicted, color='blue', label = 'Predicted Lotto Numbers')\n",
    "plt.title('Lotto Numbers Prediction')\n",
    "plt.xlabel('Lotto Results')\n",
    "plt.ylabel('Lotto Numbers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_lotto_numbers</th>\n",
       "      <th>sample_lotto_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8562</th>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8563</th>\n",
       "      <td>8</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>18</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>36</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>48</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>53</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>16</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>17</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>49</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>55</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8576</th>\n",
       "      <td>21</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>25</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>45</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8579</th>\n",
       "      <td>54</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8580</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8581</th>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>13</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>27</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>31</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>58</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>22</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>23</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>28</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>39</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>51</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>52</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9732</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9733</th>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>16</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>25</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>29</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>56</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>15</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>21</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>23</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>41</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>50</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9745</th>\n",
       "      <td>15</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9746</th>\n",
       "      <td>24</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9747</th>\n",
       "      <td>33</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>37</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>58</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>38</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>50</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9754</th>\n",
       "      <td>53</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>57</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>7</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>12</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>17</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9760</th>\n",
       "      <td>26</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>55</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real_lotto_numbers  sample_lotto_numbers\n",
       "8562                   5                   7.0\n",
       "8563                   8                  11.0\n",
       "8564                  18                  15.0\n",
       "8565                  36                  26.0\n",
       "8566                  48                  42.0\n",
       "8567                  53                  51.0\n",
       "8568                  15                   7.0\n",
       "8569                  16                  18.0\n",
       "8570                  17                  22.0\n",
       "8571                  26                  24.0\n",
       "8572                  49                  35.0\n",
       "8573                  55                  50.0\n",
       "8574                   2                   7.0\n",
       "8575                   9                  10.0\n",
       "8576                  21                  16.0\n",
       "8577                  25                  28.0\n",
       "8578                  45                  34.0\n",
       "8579                  54                  50.0\n",
       "8580                   3                   7.0\n",
       "8581                   6                  11.0\n",
       "8582                  13                  15.0\n",
       "8583                  27                  22.0\n",
       "8584                  31                  37.0\n",
       "8585                  58                  42.0\n",
       "8586                  22                   6.0\n",
       "8587                  23                  22.0\n",
       "8588                  28                  27.0\n",
       "8589                  39                  34.0\n",
       "8590                  51                  43.0\n",
       "8591                  52                  51.0\n",
       "...                  ...                   ...\n",
       "9732                   1                   7.0\n",
       "9733                   4                  10.0\n",
       "9734                  16                  14.0\n",
       "9735                  25                  24.0\n",
       "9736                  29                  35.0\n",
       "9737                  56                  41.0\n",
       "9738                  14                   6.0\n",
       "9739                  15                  17.0\n",
       "9740                  21                  20.0\n",
       "9741                  23                  28.0\n",
       "9742                  41                  33.0\n",
       "9743                  50                  46.0\n",
       "9744                   4                   7.0\n",
       "9745                  15                  11.0\n",
       "9746                  24                  21.0\n",
       "9747                  33                  31.0\n",
       "9748                  37                  39.0\n",
       "9749                  58                  45.0\n",
       "9750                   2                   6.0\n",
       "9751                  10                  10.0\n",
       "9752                  38                  16.0\n",
       "9753                  50                  40.0\n",
       "9754                  53                  49.0\n",
       "9755                  57                  54.0\n",
       "9756                   4                   7.0\n",
       "9757                   7                  11.0\n",
       "9758                  12                  15.0\n",
       "9759                  17                  21.0\n",
       "9760                  26                  28.0\n",
       "9761                  55                  40.0\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "       \n",
    "real_lotto_numbers_df = pd.Series(y_actual.reshape((9762)), name='real_lotto_numbers')\n",
    "sample_lotto_numbers_df = pd.Series(np.around(y_predicted.reshape((9762))), name='sample_lotto_numbers')\n",
    "comparison_df = pd.concat([real_lotto_numbers_df, sample_lotto_numbers_df], axis=1)\n",
    "comparison_df.tail(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_test = []\n",
    "# take all rows less from total for testing and rest use for training\n",
    "for col in pd_testing_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_test.append([row])\n",
    "#         print (row)\n",
    "\n",
    "two_dim_lotto_array_test.append([55])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9769, 1)\n",
      "[[ 8]\n",
      " [13]\n",
      " [26]\n",
      " ..., \n",
      " [26]\n",
      " [55]\n",
      " [55]]\n"
     ]
    }
   ],
   "source": [
    "# convert python list into numpy array\n",
    "testing_set = np.array(two_dim_lotto_array_test, ndmin=2)\n",
    "\n",
    "print (type(testing_set))\n",
    "print (testing_set.ndim)\n",
    "print (testing_set.shape)\n",
    "print (testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9769, 1)\n",
      "inputs [[-0.73333333]\n",
      " [-0.56666667]\n",
      " [-0.13333333]\n",
      " ..., \n",
      " [-0.13333333]\n",
      " [ 0.83333333]\n",
      " [ 0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "sc = RobustScaler()\n",
    "scaled_testing_set = sc.fit_transform(testing_set)\n",
    "\n",
    "# scaled_testing_set = testing_set\n",
    "\n",
    "print (\"inputs.shape\", scaled_testing_set.shape)\n",
    "print (\"inputs\", scaled_testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape (9763, 6)\n",
      "inputs [[-0.73333333 -0.56666667 -0.13333333  0.16666667  0.5         0.7       ]\n",
      " [-0.56666667 -0.13333333  0.16666667  0.5         0.7        -0.96666667]\n",
      " [-0.13333333  0.16666667  0.5         0.7        -0.96666667 -0.5       ]\n",
      " ..., \n",
      " [ 0.76666667  0.9        -0.86666667 -0.76666667 -0.6        -0.43333333]\n",
      " [ 0.9        -0.86666667 -0.76666667 -0.6        -0.43333333 -0.13333333]\n",
      " [-0.86666667 -0.76666667 -0.6        -0.43333333 -0.13333333  0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(6, testing_set.shape[0]):\n",
    "    inputs.append(scaled_testing_set[i-6:i, 0])\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "\n",
    "print (\"inputs.shape\", inputs.shape)\n",
    "print (\"inputs\", inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_inputs.shape (9763, 6, 1)\n",
      "test_inputs [[[-0.73333333]\n",
      "  [-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]]\n",
      "\n",
      " [[-0.56666667]\n",
      "  [-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]]\n",
      "\n",
      " [[-0.13333333]\n",
      "  [ 0.16666667]\n",
      "  [ 0.5       ]\n",
      "  [ 0.7       ]\n",
      "  [-0.96666667]\n",
      "  [-0.5       ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.76666667]\n",
      "  [ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]]\n",
      "\n",
      " [[ 0.9       ]\n",
      "  [-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]\n",
      "  [-0.13333333]]\n",
      "\n",
      " [[-0.86666667]\n",
      "  [-0.76666667]\n",
      "  [-0.6       ]\n",
      "  [-0.43333333]\n",
      "  [-0.13333333]\n",
      "  [ 0.83333333]]]\n"
     ]
    }
   ],
   "source": [
    "# reshape input to fit keras predicted model format of 3-d\n",
    "test_inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1],1))\n",
    "\n",
    "print (\"test_inputs.shape\", test_inputs.shape)\n",
    "print (\"test_inputs\", test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(9763, 1)\n",
      "[[  6.91761398]\n",
      " [ 10.01141357]\n",
      " [ 20.58066559]\n",
      " ..., \n",
      " [ 28.22393036]\n",
      " [ 39.52734375]\n",
      " [  6.16176796]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's predict\n",
    "predicted_lotto_numbers = model.predict(test_inputs)\n",
    "\n",
    "# convert prediction back to normal values \n",
    "predicted_lotto_numbers = sc.inverse_transform(predicted_lotto_numbers)\n",
    "\n",
    "print (type(predicted_lotto_numbers))\n",
    "print (predicted_lotto_numbers.ndim)\n",
    "print (predicted_lotto_numbers.shape)\n",
    "print (predicted_lotto_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "(9763,)\n",
      "[  7.  10.  21. ...,  28.  40.   6.]\n"
     ]
    }
   ],
   "source": [
    "# round reshaping back  y_predicted\n",
    "y_predicted = np.around(predicted_lotto_numbers)\n",
    "y_predicted = np.reshape(y_predicted, (inputs.shape[0]))\n",
    "print (type(y_predicted))\n",
    "print (y_predicted.ndim)\n",
    "print (y_predicted.shape)\n",
    "print (y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_lotto_numbers</th>\n",
       "      <th>sample_lotto_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8563</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>36.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>48.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>53.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>49.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>55.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8576</th>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8577</th>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8578</th>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8579</th>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8580</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8581</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8582</th>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>58.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>39.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>51.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>52.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9733</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9734</th>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>25.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>29.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>56.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>41.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9745</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9746</th>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9747</th>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>58.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9753</th>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9754</th>\n",
       "      <td>53.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>57.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9759</th>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9760</th>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>55.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9762</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real_lotto_numbers  sample_lotto_numbers\n",
       "8563                 8.0                  11.0\n",
       "8564                18.0                  15.0\n",
       "8565                36.0                  26.0\n",
       "8566                48.0                  42.0\n",
       "8567                53.0                  51.0\n",
       "8568                15.0                   7.0\n",
       "8569                16.0                  18.0\n",
       "8570                17.0                  22.0\n",
       "8571                26.0                  24.0\n",
       "8572                49.0                  35.0\n",
       "8573                55.0                  50.0\n",
       "8574                 2.0                   7.0\n",
       "8575                 9.0                  10.0\n",
       "8576                21.0                  16.0\n",
       "8577                25.0                  28.0\n",
       "8578                45.0                  34.0\n",
       "8579                54.0                  50.0\n",
       "8580                 3.0                   7.0\n",
       "8581                 6.0                  11.0\n",
       "8582                13.0                  15.0\n",
       "8583                27.0                  22.0\n",
       "8584                31.0                  37.0\n",
       "8585                58.0                  42.0\n",
       "8586                22.0                   6.0\n",
       "8587                23.0                  22.0\n",
       "8588                28.0                  27.0\n",
       "8589                39.0                  34.0\n",
       "8590                51.0                  43.0\n",
       "8591                52.0                  51.0\n",
       "8592                16.0                   8.0\n",
       "...                  ...                   ...\n",
       "9733                 4.0                  10.0\n",
       "9734                16.0                  14.0\n",
       "9735                25.0                  24.0\n",
       "9736                29.0                  35.0\n",
       "9737                56.0                  41.0\n",
       "9738                14.0                   6.0\n",
       "9739                15.0                  17.0\n",
       "9740                21.0                  20.0\n",
       "9741                23.0                  28.0\n",
       "9742                41.0                  33.0\n",
       "9743                50.0                  46.0\n",
       "9744                 4.0                   7.0\n",
       "9745                15.0                  11.0\n",
       "9746                24.0                  21.0\n",
       "9747                33.0                  31.0\n",
       "9748                37.0                  39.0\n",
       "9749                58.0                  45.0\n",
       "9750                 2.0                   6.0\n",
       "9751                10.0                  10.0\n",
       "9752                38.0                  16.0\n",
       "9753                50.0                  40.0\n",
       "9754                53.0                  49.0\n",
       "9755                57.0                  54.0\n",
       "9756                 4.0                   7.0\n",
       "9757                 7.0                  11.0\n",
       "9758                12.0                  15.0\n",
       "9759                17.0                  21.0\n",
       "9760                26.0                  28.0\n",
       "9761                55.0                  40.0\n",
       "9762                 NaN                   6.0\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_lotto_numbers_df = pd.Series(y_actual.reshape((9762)), name='real_lotto_numbers')\n",
    "sample_lotto_numbers_df = pd.Series(np.around(y_predicted.reshape((testing_set.shape[0]-6))), name='sample_lotto_numbers')\n",
    "comparison_df = pd.concat([real_lotto_numbers_df, sample_lotto_numbers_df], axis=1)\n",
    "comparison_df.tail(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "# now add each column of six rows into next 6 rows of first column\n",
    "two_dim_lotto_array_test = []\n",
    "# take all rows less from total for testing and rest use for training\n",
    "for col in pd_testing_set.iloc[:,:].values:\n",
    "#     print (col)\n",
    "    for row in col:\n",
    "        two_dim_lotto_array_test.append([row])\n",
    "#         print (row)\n",
    "\n",
    "two_dim_lotto_array_test.append([55])\n",
    "\n",
    "# convert python list into numpy array\n",
    "testing_set = np.array(two_dim_lotto_array_test, ndmin=2)\n",
    "\n",
    "#scaling\n",
    "scaled_testing_set = sc.fit_transform(testing_set)\n",
    "\n",
    "# Creating a data structure with 60 timesteps and t+1 output\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(6, testing_set.shape[0]):\n",
    "    inputs.append(scaled_testing_set[i-6:i, 0])\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "\n",
    "\n",
    "# reshape input to fit keras predicted model format of 3-d\n",
    "test_inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1],1))\n",
    "\n",
    "# Now, let's predict\n",
    "predicted_lotto_numbers = model.predict(test_inputs)\n",
    "\n",
    "# convert prediction back to normal values \n",
    "predicted_lotto_numbers = sc.inverse_transform(predicted_lotto_numbers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
